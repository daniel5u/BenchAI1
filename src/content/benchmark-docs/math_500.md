## Overview

MATH-500 is a curated subset of the original MATH dataset, which was created by OpenAI to benchmark mathematical reasoning capabilities of AI models. The 500-problem selection represents challenging competition-level mathematics that requires sophisticated problem-solving skills.

## Domain Coverage

The benchmark covers **six major mathematical domains**:

| Domain | Topics Included |
|--------|---------------|
| **Algebra** | Equations, polynomials, sequences, functions |
| **Geometry** | Shapes, proofs, spatial reasoning, theorems |
| **Number Theory** | Divisibility, modular arithmetic, prime numbers |
| **Counting & Probability** | Combinatorics, probability distributions |
| **Precalculus** | Limits, derivatives, complex numbers |
| **Intermediate Algebra** | Advanced algebraic manipulations |

## Difficulty Level

Problems in MATH-500 are sourced from:

- **Math competitions**: AIME, AMC, and similar level challenges
- **Textbook exercises**: Challenging problems from advanced math texts
- **Competition preparation materials**: Problems designed to train competitive mathematicians

## Evaluation Approach

Models are evaluated based on:

- **Numerical Accuracy**: Answers must be exact (typically integers or simplified fractions)
- **Step-by-step reasoning**: Models show their work
- **Multiple solution paths**: Some problems have elegant and brute-force solutions

## Purpose

MATH-500 tests whether AI models can:

- Perform complex mathematical calculations
- Apply mathematical theorems and properties
- Think creatively about problem-solving
- Present clear mathematical reasoning

This benchmark is particularly important because mathematics requires both computational accuracy and abstract reasoning - capabilities that distinguish sophisticated AI systems.

---

*Source: [OpenAI](https://github.com/openai/prm800k)*
